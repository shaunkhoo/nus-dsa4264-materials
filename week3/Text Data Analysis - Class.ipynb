{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e5faf5-fd1a-450b-a3e4-81b005263593",
   "metadata": {},
   "source": [
    "# Text Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6911bf9-9598-466a-9fe0-7c1a26c96ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import flair\n",
    "import gensim\n",
    "import umap\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb8c4c2-be97-412f-9ecd-64652e3f4e00",
   "metadata": {},
   "source": [
    "## 1. Hansard Data\n",
    "\n",
    "In this section, we explore Hansard data, which consists of speeches and debates made in Singapore's Parliament Chamber and provides a record of parliamentary business and proceedings in a Sitting. Data from Hansard has already been scraped for you, focusing specifically on the Committee of Supply (or Budget) debates for the 14th Parliament (from 2021 to present). We will use this as an opportunity to explore sentiment analysis and topic modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0084219-821a-458e-ac35-b98d09e55347",
   "metadata": {},
   "source": [
    "### 1.1 Importing the data and doing simple processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6298ec-66c3-43ad-b1d2-89ffaca14cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the Hansard data\n",
    "hansard_df = pd.read_csv(\"Hansard_15th_Parl_COS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa48c668-282a-4d24-88a0-86e1296e15d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hansard_df.shape)\n",
    "hansard_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e73e1c-184d-409b-9437-5a3e42a245ab",
   "metadata": {},
   "source": [
    "Let's try to enrich this dataset with some useful variables. \n",
    "\n",
    "<span style=\"background-color: #FFFF00; color: #000000\">**Exercise:** Create two new columns for this dataset: \n",
    "* `Sitting Year` (int): Year in which the speech was given\n",
    "* `Speech Length` (int): Number of words in the speech </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f31692-fc46-4e05-a2d0-729b46529b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970bed0b-2468-4abd-acb4-d605c1e62819",
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba0438f-cccf-43c4-a53a-394890da4b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_df['Speech Length'].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee6d5f0-b140-406c-87b0-049ad9a77bfa",
   "metadata": {},
   "source": [
    "### 1.2 Sentiment Analysis\n",
    "\n",
    "Let's start with applying some sentiment analysis. While most Parliamentary speeches are likely to be quite mild in terms of sentiment, we might be able to identify some more impassioned speeches. Before you proceed, make sure you have both the `spacy` and `spacytextblob` libraries installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42817352-c78c-411c-a495-c17f73e83e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the command here to download textblob's additional corpuses\n",
    "!python -m textblob.download_corpora\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0381cd9c-fdb7-4ef3-8b7b-7ab3fe5aa802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "# Initialise the NLP pipeline and add the spacetextblob step to the pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "# Add our texts\n",
    "texts = hansard_df['Speech']\n",
    "\n",
    "# This will take about 20-30 seconds to run\n",
    "sentiment_results = []\n",
    "for doc in nlp.pipe(texts, disable=[\"tok2vec\", \"tagger\", \"ner\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "    sentiment_results.append({\n",
    "        'Polarity': doc._.blob.polarity,\n",
    "        'Subjectivity': doc._.blob.subjectivity, \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc179a-6546-4047-a11a-c6998bd2f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we append it to our original dataset\n",
    "sentiment_results_df = pd.DataFrame(sentiment_results)\n",
    "hansard_df = pd.concat([hansard_df, sentiment_results_df], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192ec0d-fb6c-43ef-824b-648493d07137",
   "metadata": {},
   "source": [
    "Let's do some simple data analysis to understand the distributions of the polarity and subjectivity scores. Before you run the cells below, think carefully about what you expect from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a9f74-2ae8-4f51-a7a7-50df4bcac3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polarity scores range from [-1 to 1], with -1 indicating very negative and 1 indicating very positive\n",
    "hansard_df['Polarity'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae48303-fe16-4db6-9af9-b054374c5075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjectivity scores range from [0 to 1], with 0 indicating very objective and 1 indicating very subjective\n",
    "hansard_df['Subjectivity'].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be0805e-0674-40c6-8839-7b28e3bc0fb9",
   "metadata": {},
   "source": [
    "Unsurprisingly most texts are neutral and objective. But this may be swayed by the number of times the Chairman speaks. Let's filter that out and look at this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073d788b-ef87-4a64-aec6-24b0b83bfb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_df_cleaned = hansard_df[hansard_df['Speaker'] != \"The Chairman\"].reset_index(drop = True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30def40e-b6e6-4864-9488-b2cb06d94913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polarity scores range from [-1 to 1], with -1 indicating very negative and 1 indicating very positive\n",
    "hansard_df_cleaned['Polarity'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c555e1ab-056b-4b88-b153-3b414b8fbb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjectivity scores range from [0 to 1], with 0 indicating very objective and 1 indicating very subjective\n",
    "hansard_df_cleaned['Subjectivity'].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fbf6b2-bd63-4780-addc-218e7418a8be",
   "metadata": {},
   "source": [
    "Let's find the most positive speech and the most negative speech! Share your thoughts about the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18079f6b-2111-4204-92cb-e6714f9cf010",
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_df_cleaned.loc[hansard_df_cleaned['Polarity'].idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d564699-1ef8-42eb-8685-c856fa61ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_df_cleaned.loc[hansard_df_cleaned['Polarity'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ff8e5-9f55-4f66-a9d2-8fb72932b7fb",
   "metadata": {},
   "source": [
    "Both of these speeches seem a bit short, which might explain their extreme polarity scores. Let's plot a scatter plot to highlight the relationship between speech length and polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7999be57-72f2-48d4-9c70-086935a3314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_df_cleaned.plot.scatter(x = 'Speech Length', y = 'Polarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98306807-f285-47f6-ba9b-43cc225c9405",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFF00; color: #000000\">**Class Discussion:** Given that `textblob` is a dictionary-based approach to sentiment analysis, can you think of why longer speeches tend to have less extreme values for positive/negative sentiment?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2f7ce8-0e51-4d3f-acd9-ed5871362d3e",
   "metadata": {},
   "source": [
    "Now let's try a different approach: using an embedding-based classifier instead! We will use a small embedding-based classifier that has already been finetuned to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423ba3a-fb13-4002-b256-7891a660e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.nn import Classifier\n",
    "from flair.data import Sentence\n",
    "tagger = Classifier.load('./flair_sentiment.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2363c0e2-ab5c-4857-b2e1-e35e475ac266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try it out with a random speech\n",
    "sentence = Sentence(hansard_df_cleaned['Speech'][2])\n",
    "tagger.predict(sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff1ee5-fdac-4661-9515-24292a166325",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = []\n",
    "\n",
    "# This should take around 3-5 minutes\n",
    "for text in tqdm(hansard_df_cleaned['Speech'].tolist()):\n",
    "    sentence = Sentence(text)\n",
    "    tagger.predict(sentence)\n",
    "\n",
    "    # Remember to take the inverse of the negative score\n",
    "    if sentence.labels[0].value == 'NEGATIVE':\n",
    "        sentiment_scores.append(1 - sentence.labels[0].score)\n",
    "    else:\n",
    "        sentiment_scores.append(sentence.labels[0].score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e053f049-2e44-426b-b37e-79f42e0f7047",
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_df_cleaned['Sentiment'] = sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70440f86-23fa-4fc8-ae70-e2bfa8c9dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_df_cleaned['Sentiment'].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2f49f-39c7-402d-aa94-6e248ab60c8b",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFF00; color: #000000\">**Class Discussion:** What do you notice about this chart that is different from the `textblob` model results? Why do you think there is such a big difference?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bed5275-df94-4cd5-b6d3-1f21c637d689",
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_df_cleaned.plot.scatter(x = 'Speech Length', y = 'Sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913a9582-ad32-4219-b8f2-2ac774d4773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_df_cleaned.loc[hansard_df_cleaned['Sentiment'].idxmin()]['Speech']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113659c6-b7f2-4d40-9fb5-47c2666ad090",
   "metadata": {},
   "source": [
    "### 1.3 Topic modelling\n",
    "\n",
    "Since Parliamentary debates tend to be quite topic-focused, topic modelling would be a good option for us to better understand the ongoing debates and to get a sense of the priority areas for discussion in Parliament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b32a912-00e8-46a5-8df6-b6942e30a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['also', 'mr', 'chairman', 'beg', 'move'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenization\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove punctuation and non-alphabetic tokens\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    \n",
    "    # Stopword removal and lemmatization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf066531-9cac-4007-84d2-98ee2ea2be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Step 1: Preprocessing (with stopword removal and lemmatization)\n",
    "texts_preprocessed = [preprocess(text) for text in hansard_df_cleaned['Speech']]\n",
    "\n",
    "# Step 2: Vectorizing the text data\n",
    "vectorizer = CountVectorizer()\n",
    "dtm = vectorizer.fit_transform(texts_preprocessed)\n",
    "\n",
    "# Step 3: Applying LDA for Topic Modeling\n",
    "lda = LatentDirichletAllocation(n_components = 10, random_state = 2024)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Step 4: Extracting and Displaying Topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\", \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "tf_feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2bce67-7b58-400a-b9e2-119d4bed8cbc",
   "metadata": {},
   "source": [
    "The topics looks fairly sensible, but is there a way for us to get a more tangible and concrete way to assess the quality of this topic modelling? We can look at the **coherence score** for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a321fda6-b480-4a88-9567-b95fb78765e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Step 1: Create a Gensim Dictionary and Corpus\n",
    "texts_tokenized = [text.split() for text in texts_preprocessed]\n",
    "dictionary = Dictionary(texts_tokenized)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_tokenized]\n",
    "\n",
    "# Step 2: Get the topics from the LDA model\n",
    "lda_topics = lda.components_\n",
    "lda_topics_words = [[vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]] for topic in lda_topics]\n",
    "\n",
    "# Step 3: Calculate Coherence Score\n",
    "coherence_model_lda = CoherenceModel(topics = lda_topics_words, \n",
    "                                     texts = texts_tokenized, \n",
    "                                     dictionary = dictionary, \n",
    "                                     coherence = 'c_v')\n",
    "\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f'Coherence Score for LDA Model: {coherence_lda}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4d11b-1e3e-49e8-a31d-b6b62602edf3",
   "metadata": {},
   "source": [
    "Now let's try varying some of the parameters to see which gets us the optimal coherence score. We'll start by adjusting the number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871df834-0e55-49dc-ae54-3fc11fb18cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics_list = [3, 5, 10, 15, 20, 25]\n",
    "coherence_scores = []\n",
    "\n",
    "# It should take around 15-30 seconds for each iteration\n",
    "for n_topics in tqdm(n_topics_list):\n",
    "        \n",
    "    lda = LatentDirichletAllocation(n_components = n_topics, random_state = 2024)\n",
    "    lda.fit(dtm)\n",
    "    lda_topics = lda.components_\n",
    "    lda_topics_words = [[vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]] for topic in lda_topics]    \n",
    "    coherence_model_lda = CoherenceModel(topics = lda_topics_words, \n",
    "                                         texts = texts_tokenized, \n",
    "                                         dictionary = dictionary, \n",
    "                                         coherence = 'c_v')    \n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print(f\"Number of topics: {n_topics} | Coherence Score: {coherence_lda}\")\n",
    "    coherence_scores.append(coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4743ec4-dae8-446d-a756-4f884920af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(n_topics_list, coherence_scores)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087e977d-8fdc-40de-8270-bb8c852f3ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components = 20, random_state = 2024)\n",
    "lda.fit(dtm)\n",
    "no_top_words = 10\n",
    "tf_feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4421fd-e841-4a45-b539-d0bf0ae4da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the topic distribution for each document\n",
    "lda_topic_distributions = lda.transform(dtm)\n",
    "\n",
    "# Step 2: Identify the dominant topic for each document\n",
    "dominant_topics = np.argmax(lda_topic_distributions, axis = 1)\n",
    "\n",
    "# Step 3: Apply UMAP to reduce to 2 dimensions\n",
    "umap_model = umap.UMAP(n_components = 2, random_state = 2024)\n",
    "lda_2d = umap_model.fit_transform(lda_topic_distributions)\n",
    "\n",
    "# Step 4: Prepare data for Plotly\n",
    "df = pd.DataFrame({\n",
    "    'UMAP1': lda_2d[:, 0],\n",
    "    'UMAP2': lda_2d[:, 1],\n",
    "    'Dominant Topic': dominant_topics,\n",
    "    'Text': hansard_df_cleaned['Speech'].str.slice(0,1000).tolist()  \n",
    "})\n",
    "\n",
    "# Step 5: Create custom hover template to control text width\n",
    "hover_template = '<br>'.join(['%{customdata}'])\n",
    "\n",
    "# Limiting the line width by adding line breaks after a specific number of characters (e.g., 50)\n",
    "df['Text'] = df['Text'].apply(lambda x: '<br>'.join([x[i:i+50] for i in range(0, len(x), 50)]))\n",
    "\n",
    "# Step 6: Create an interactive plot with Plotly\n",
    "fig = px.scatter(\n",
    "    df, x='UMAP1', y='UMAP2',\n",
    "    color='Dominant Topic',\n",
    "    custom_data=['Text'],  # Use custom data for hover template\n",
    "    title='Interactive UMAP Projection of LDA Topic Distributions',\n",
    "    color_continuous_scale=px.colors.qualitative.Set1\n",
    ")\n",
    "\n",
    "# Customize hover template to use our custom text formatting\n",
    "fig.update_traces(\n",
    "    hovertemplate=hover_template,\n",
    "    marker=dict(size=8, opacity=0.7)\n",
    ")\n",
    "\n",
    "# Customize layout with specific dimensions\n",
    "fig.update_layout(\n",
    "    width = 1200,\n",
    "    height = 800,\n",
    "    legend_title_text='Dominant Topic',\n",
    "    legend = dict(\n",
    "        itemsizing='constant'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d82a23-325d-4c20-81a0-4146e49edeac",
   "metadata": {},
   "source": [
    "Now we try with another topic modelling approach: using embeddings with BERTopic. Note that BERTopic relies on hierarchical clustering, so we don't have to set any number of topics as a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3db7579-c920-4db4-8a9e-f11b58bfe25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Step 1: Initialize BERTopic\n",
    "topic_model = BERTopic()\n",
    "\n",
    "# Step 2: Fit the model to your data\n",
    "topics, probabilities = topic_model.fit_transform(hansard_df_cleaned['Speech'].tolist())\n",
    "\n",
    "# Step 3: View the topics\n",
    "topics_overview = topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500d621c-ce2d-48d0-a103-49fc7f236ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c9625-cff4-40b2-9521-02920d67288d",
   "metadata": {},
   "source": [
    "BERTopic says we have 74 topics, which sounds like a lot of topics compared to what we had previously! Unfortunately it also seems like 698 (or about 20% of the data) are considered as \"outliers\". Let's use some of the data visualisation tools to get a visual appreciation of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed95cfa-3b85-42cd-820e-2152c6efa323",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be955b6-2bd1-4165-8df6-86836ae0e8e8",
   "metadata": {},
   "source": [
    "It seems like some of these clusters overlap a lot. What if we looked at the documents and topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc0dd0a-efe3-47f0-ae7e-2f9687710bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_documents(hansard_df_cleaned['Speech'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153e1f11-f169-4d96-b703-97172e1b375c",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFF00; color: #000000\">**Class Discussion:** What are your observations about the quality of the topics identified here, versus the topics identified by the LDA model? Are there significant differences, and if so, in what ways?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f8eff-ed26-42cc-b281-9fc6e64d1e17",
   "metadata": {},
   "source": [
    "## 2. NUS SMS Data\n",
    "\n",
    "In this section we explore the NUS SMS corpus that was released [here](https://github.com/kite1988/nus-sms-corpus), mainly to demonstrate the challenges of analysing Singlish data and how conventional NLP techniques may fail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfac36f9-03c6-4d73-b927-a3f4f357d38d",
   "metadata": {},
   "source": [
    "### 2.1 Importing the data and doing simple processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418538e1-ad13-499a-bf28-eb6762dae27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"smsCorpus_en_2015.03.09_all.json\", 'r') as file:\n",
    "    sms_corpus = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddb6394-c19e-4797-96e5-624026d08e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many messages there are in this corpus\n",
    "len(sms_corpus['smsCorpus']['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4753b3-92d8-43ae-89d4-aaf764f030ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the first message\n",
    "sms_corpus['smsCorpus']['message'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619d01ce-43ef-4a1d-ac86-9bc787606b58",
   "metadata": {},
   "source": [
    "Now we write a function to extract all the SMSes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517e5412-d458-4661-8758-dc7172f0877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_corpus_list = []\n",
    "for message in sms_corpus['smsCorpus']['message']:\n",
    "    sms_corpus_list.append({\n",
    "        'ID': message['@id'],\n",
    "        'Text': message['text']['$']\n",
    "    })\n",
    "sms_corpus_df = pd.DataFrame(sms_corpus_list)\n",
    "sms_corpus_df['Text'] = sms_corpus_df['Text'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c8971-c075-4034-a6f5-a60e279e9be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cacdea-8de5-42c8-8861-781007629025",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFF00; color: #000000\">**Exercise:** Create two new columns for this dataset:  </span>\n",
    "* Word Count (int): How many words are in the text\n",
    "* Polarity (float): How positive or negative the text is (using `textblob` and `spacy`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a5566-64de-4259-acd1-f98dbb77ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0ace2-f94f-4fa4-8bdd-5bf822f586dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_corpus_df['Word Count'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f65f0d-079e-47a2-9071-35d024daedb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_corpus_df['Polarity'] = sentiment_results\n",
    "sms_corpus_df['Polarity'].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab4ffa8-9f2f-46a7-909c-eb5db17ba3dd",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFF00; color: #000000\">**Class Discussion:** Before you ran these plots, what were you expecting? Now after having seen these plots, what are your thoughts? Is this what you had expected, and why?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28d5c71-8b19-4204-9f0a-412e293a25b5",
   "metadata": {},
   "source": [
    "### 2.2: Topic modelling\n",
    "\n",
    "We try with some topic modelling to highlight the challenges of topic modelling with short texts, on top of the difficulties with Singlish texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a661e6-f6af-4d5e-a561-8a3748b7b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocessing (with stopword removal and lemmatization)\n",
    "texts_preprocessed = [preprocess(text) for text in sms_corpus_df['Text'].astype('str')]\n",
    "\n",
    "# Step 2: Vectorizing the text data\n",
    "vectorizer = CountVectorizer()\n",
    "dtm = vectorizer.fit_transform(texts_preprocessed)\n",
    "\n",
    "# Step 3: Applying LDA for Topic Modeling\n",
    "lda = LatentDirichletAllocation(n_components = 10, random_state = 2024)\n",
    "lda.fit(dtm)\n",
    "\n",
    "no_top_words = 10\n",
    "tf_feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eed1691-c2c3-4d07-ad3e-7ec17b660c39",
   "metadata": {},
   "source": [
    "The topics here look quite bad, but this is unsurprising given how short SMSes are. Topic modelling tends to underperform in these cases. We check this by computing the coherence score as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883f91c-eede-482c-8221-ccf4196c9557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a Gensim Dictionary and Corpus\n",
    "texts_tokenized = [text.split() for text in texts_preprocessed]\n",
    "dictionary = Dictionary(texts_tokenized)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_tokenized]\n",
    "\n",
    "# Step 2: Get the topics from the LDA model\n",
    "lda_topics = lda.components_\n",
    "lda_topics_words = [[vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]] for topic in lda_topics]\n",
    "\n",
    "# Step 3: Calculate Coherence Score\n",
    "coherence_model_lda = CoherenceModel(topics = lda_topics_words, \n",
    "                                     texts = texts_tokenized, \n",
    "                                     dictionary = dictionary, \n",
    "                                     coherence = 'c_v')\n",
    "\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f'Coherence Score for LDA Model: {coherence_lda}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb823f-1533-4235-8adc-b8e23133df62",
   "metadata": {},
   "source": [
    "One problem with Singlish is the difficulty in tokenising it correctly. Let's take a look by applying BERT's tokeniser to some of the Singlish texts here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b793e-17ae-4521-9000-d41b38610b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a5877-7401-4fe0-959d-5f13a8ec42bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sms_corpus_df['Text'][0])\n",
    "tokenizer.tokenize(sms_corpus_df['Text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd872c2d-54dd-4191-829b-e85ccc1800fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sms_corpus_df['Text'][36])\n",
    "tokenizer.tokenize(sms_corpus_df['Text'][36])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wbh0uhm23l",
   "metadata": {},
   "source": [
    "## 3. Introduction to Large Language Models\n",
    "\n",
    "In this section, we will use a LLM, specifically Google's **Gemini** model (free tier), to perform the same tasks we did earlier — summarisation, topic classification, and sentiment analysis — and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9hdp3o0z72a",
   "metadata": {},
   "source": [
    "### 3.1 Setup\n",
    "\n",
    "To use Google's Gemini API, you will need to:\n",
    "1. Go to [Google AI Studio](https://aistudio.google.com) and sign in with your Google account\n",
    "2. Generate an API key\n",
    "3. Create a `.env` file in this directory with the line: `GEMINI_API_KEY=your_key_here`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a50870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcxzpyo6o7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "# The client automatically picks up GEMINI_API_KEY from the environment\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# We'll use Gemini 2.0 Flash — fast, capable, and free\n",
    "MODEL_ID = \"gemini-3-flash-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "firtb22w3w7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test — make sure the API is working\n",
    "response = client.models.generate_content(model=MODEL_ID, contents=\"Say hello in one sentence.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdx0fb5suy",
   "metadata": {},
   "source": [
    "### 3.2 Summarising Parliamentary Speeches\n",
    "\n",
    "One of the most immediately useful capabilities of LLMs is summarisation. Let's take a long parliamentary speech and ask Gemini to summarise it. Compare how easy this is versus building a custom extractive or abstractive summariser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44q8lums0s2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one of the longest speeches in the dataset\n",
    "long_speech_idx = hansard_df_cleaned['Speech Length'].idxmax()\n",
    "long_speech = hansard_df_cleaned.loc[long_speech_idx]\n",
    "\n",
    "print(f\"Speaker: {long_speech['Speaker']}\")\n",
    "print(f\"Title: {long_speech['Title']}\")\n",
    "print(f\"Word count: {long_speech['Speech Length']}\")\n",
    "print(f\"\\nFirst 500 characters:\\n{long_speech['Speech'][:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pjtuxxwjrz",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Summarise the following parliamentary speech in 3-5 bullet points. \n",
    "Focus on the key policy issues raised and any questions asked.\n",
    "\n",
    "Speech:\n",
    "{long_speech['Speech']}\"\"\"\n",
    "\n",
    "response = client.models.generate_content(model=MODEL_ID, contents=prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vrcbpmnukm",
   "metadata": {},
   "source": [
    "### 3.3 Zero-Shot Topic Classification\n",
    "\n",
    "Earlier, we used LDA to assign topics to speeches. This required preprocessing, vectorisation, and hyperparameter tuning. With an LLM, we can simply *describe* the topics and ask the model to classify — no training required. This is called **zero-shot classification**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051cftnbsdew",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define topic labels based on the 20-topic LDA results from Section 1.3\n",
    "topic_labels = [\n",
    "    \"Parliamentary procedure / general debate\",\n",
    "    \"Community and youth programmes\",\n",
    "    \"Arts, sports, and heritage\",\n",
    "    \"Neighbourhood and resident issues\",\n",
    "    \"Foreign affairs and defence (ASEAN)\",\n",
    "    \"Public governance and women's issues\",\n",
    "    \"Transport and public roads\",\n",
    "    \"Education and schools\",\n",
    "    \"HDB housing and rental\",\n",
    "    \"Digital services and smart nation\",\n",
    "    \"Government procurement and tax\",\n",
    "    \"Business, enterprise, and economy\",\n",
    "    \"Food, hawkers, and climate/environment\",\n",
    "    \"Healthcare and family support\",\n",
    "    \"Workers, jobs, and wages\",\n",
    "    \"Drugs, prisons, and rehabilitation\",\n",
    "    \"Vehicles and driving\",\n",
    "    \"SAF, scams, and security\",\n",
    "    \"Legal and judicial matters\",\n",
    "    \"Electric vehicles and banking\",\n",
    "]\n",
    "\n",
    "topic_list_str = \"\\n\".join([f\"{i}: {label}\" for i, label in enumerate(topic_labels)])\n",
    "print(topic_list_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nyrcfbigicr",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Sample 3 speeches with reasonable length for classification\n",
    "sample_df = hansard_df_cleaned[hansard_df_cleaned['Speech Length'] > 100].sample(3, random_state=2024).reset_index(drop=True)\n",
    "\n",
    "llm_topics = []\n",
    "for i, row in sample_df.iterrows():\n",
    "    prompt = f\"\"\"Classify the following parliamentary speech into exactly ONE of these topics. \n",
    "Respond with ONLY the topic number (0-19), nothing else.\n",
    "\n",
    "Topics:\n",
    "{topic_list_str}\n",
    "\n",
    "Speech:\n",
    "{row['Speech'][:2000]}\"\"\"\n",
    "    \n",
    "    response = client.models.generate_content(model=MODEL_ID, contents=prompt)\n",
    "    llm_topic = response.text.strip()\n",
    "    llm_topics.append(llm_topic)\n",
    "    print(f\"Speech {i}: LLM says topic {llm_topic} ({topic_labels[int(llm_topic)] if llm_topic.isdigit() else 'INVALID'})\")\n",
    "    time.sleep(5)  # Be polite to the free API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qep543u6jvg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with LDA's topic assignments for the same speeches\n",
    "# We need to re-transform these speeches through the LDA pipeline\n",
    "sample_texts_preprocessed = [preprocess(text) for text in sample_df['Speech']]\n",
    "sample_dtm = vectorizer.transform(sample_texts_preprocessed)\n",
    "sample_lda_topics = np.argmax(lda.transform(sample_dtm), axis=1)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Speech (first 80 chars)': sample_df['Speech'].str[:80],\n",
    "    'LDA Topic': [f\"{t} ({topic_labels[t]})\" for t in sample_lda_topics],\n",
    "    'LLM Topic': [f\"{t} ({topic_labels[int(t)] if t.isdigit() else 'INVALID'})\" for t in llm_topics],\n",
    "})\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mi2ye4ob21",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFF00; color: #000000\">**Exercise:** Try modifying the prompt above — for example, ask the model to also provide a one-sentence justification for its choice. How does adding instructions to the prompt change the output? Does the classification quality improve or worsen?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise3-placeholder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vnvxtvmaihs",
   "metadata": {},
   "source": [
    "### 3.4 Sentiment Analysis with an LLM\n",
    "\n",
    "A key advantage of LLMs over traditional sentiment tools is that they can **explain their reasoning**. TextBlob gives you a number; Flair gives you a label and a score. An LLM can tell you *why* it thinks a speech is positive or negative — which is far more useful for policy analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53w6xce7746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyse sentiment for the same speeches we compared earlier\n",
    "# Recall: \"Do you mind repeating? I am sorry.\" was rated most negative by TextBlob\n",
    "test_speeches = [\n",
    "    (\"TextBlob most negative\", hansard_df_cleaned.loc[hansard_df_cleaned['Polarity'].idxmin(), 'Speech']),\n",
    "    (\"TextBlob most positive\", hansard_df_cleaned.loc[hansard_df_cleaned['Polarity'].idxmax(), 'Speech']),\n",
    "    (\"Flair most negative\", hansard_df_cleaned.loc[hansard_df_cleaned['Sentiment'].idxmin(), 'Speech'][:2000]),\n",
    "]\n",
    "\n",
    "for label, speech in test_speeches:\n",
    "    prompt = f\"\"\"Analyse the sentiment of this parliamentary speech. \n",
    "Provide:\n",
    "1. A sentiment score from -1.0 (very negative) to 1.0 (very positive)\n",
    "2. A one-sentence explanation of your reasoning\n",
    "\n",
    "Format your response exactly as:\n",
    "Score: [number]\n",
    "Reason: [explanation]\n",
    "\n",
    "Speech:\n",
    "{speech}\"\"\"\n",
    "    \n",
    "    response = client.models.generate_content(model=MODEL_ID, contents=prompt)\n",
    "    print(f\"=== {label} ===\")\n",
    "    print(f\"Speech: {speech[:100]}...\")\n",
    "    print(response.text)\n",
    "    print()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bvvpwmigtzm",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFF00; color: #000000\">**Class Discussion:** Compare the LLM's sentiment analysis with TextBlob and Flair's results. Notice how the LLM correctly identifies that \"Do you mind repeating? I am sorry.\" is a neutral/polite request, not a negative statement. What does this tell us about the limitations of dictionary-based and embedding-based approaches for domain-specific text like parliamentary speeches?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ck75pojm17b",
   "metadata": {},
   "source": [
    "### 3.5 Handling Singlish with LLMs\n",
    "\n",
    "In Section 2, we saw that BERT's tokeniser struggles with Singlish — splitting words like \"Bugis\" and \"oso\" into meaningless subword tokens. The LDA topic model also produced poor results on SMS data. LLMs trained on diverse internet text (including forums, social media, and chat) tend to handle colloquial language much better. Let's test this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wvh09w92vn8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick some Singlish-heavy SMS messages\n",
    "singlish_samples = [\n",
    "    sms_corpus_df['Text'][0],   # \"Bugis oso near wat...\"\n",
    "    sms_corpus_df['Text'][3],   # \"Den only weekdays got special price... Haiz...\"\n",
    "    sms_corpus_df['Text'][36],  # \"ll go yan jiu too... We can skip ard oso...\"\n",
    "]\n",
    "\n",
    "for sms in singlish_samples:\n",
    "    prompt = f\"\"\"This is a Singlish SMS message from Singapore. Please:\n",
    "1. Translate it to standard English\n",
    "2. Rate the sentiment from -1.0 (very negative) to 1.0 (very positive)\n",
    "\n",
    "Format your response as:\n",
    "Translation: [standard English version]\n",
    "Sentiment: [score]\n",
    "\n",
    "SMS: {sms}\"\"\"\n",
    "    \n",
    "    response = client.models.generate_content(model=MODEL_ID, contents=prompt)\n",
    "    print(f\"Original: {sms}\")\n",
    "    print(response.text)\n",
    "    print()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xhe08pmx4jf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with TextBlob's polarity on the same messages\n",
    "for sms in singlish_samples:\n",
    "    doc = nlp(sms)\n",
    "    print(f\"SMS: {sms}\")\n",
    "    print(f\"TextBlob Polarity: {doc._.blob.polarity:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jzyqtyentt",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFF00; color: #000000\">**Class Discussion:** How well does the LLM handle Singlish compared to TextBlob and BERT's tokeniser? What are the implications for NLP work in Singapore's multilingual context? Think about scenarios in the public sector where you might encounter Singlish text (e.g. social media feedback, community forums, helpline transcripts).</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}